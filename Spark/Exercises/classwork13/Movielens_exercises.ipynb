{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Analytics with MovieLens Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we will use the [MovieLens 20M Dataset](https://grouplens.org/datasets/movielens/20m/) on movie ratings to answer several tasks by using `PySpark`. The exercises are structured as a guideline to get familiar with the Pyspark syntax. Have also a look on the [official pySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html). \n",
    "\n",
    "**Introduction to Movielens dataset**\n",
    "\n",
    "The Introduction exercises have the following goals:\n",
    "- Reading and understanding the schema of our movielens dataset\n",
    "- Calculating some summary statistics of our dataset\n",
    "- Learn how to perform joins and aggregations using Spark\n",
    "\n",
    "This will be also illustrated by guided exercises to get a first understanding of Spark\n",
    "- Guided Exercise 1: Which movies are the most popular ones?\n",
    "- Guided Exercise 2: What are the distinct genres in the Movielens Dataset (RDD)?\n",
    "\n",
    "**Exercises for you:**\n",
    "- Exercise 1: Which movies have the highest number of ratings?\n",
    "- Exercise 2: What's the number of movies in each genre?\n",
    "- Exercise 3: Which movies are a matter of taste?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Sparksession\n",
    "\n",
    "Execute the following cell to initialize a Sparksession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/29 21:05:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/zulu8.78.0.19-ca-jdk8.0.412-linux_aarch64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://rpi0:7077\") \\\n",
    "    .appName(\"MovieLens\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our movielens dataset contains 20 million ratings and 465'000 tag applications applied to 27'000 movies by 138'000 users. It also includs tag genome data with 12 million relevance scores across 1100 tags.\n",
    "\n",
    "The whole dataset contains six CSV files:\n",
    "- genome-scores.csv\n",
    "- genome-tags.csv\n",
    "- links.csv\n",
    "- movies.csv\n",
    "- ratings.csv\n",
    "- tags.csv\n",
    "\n",
    "In this Introduction exercise, we will have a look on the **`movies`** and **`ratings`** dataframes.\n",
    "\n",
    "To read a CSV file in our \"ml-20m\" folder, we access the `DataFrameReader` class through `read` and call the `csv()` method on it. We also specify `option(\"header\", \"true\")` since the first row of the file contains our column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|      2|   3.5|1112486027|\n",
      "|     1|     29|   3.5|1112484676|\n",
      "|     1|     32|   3.5|1112484819|\n",
      "|     1|     47|   3.5|1112484727|\n",
      "|     1|     50|   3.5|1112484580|\n",
      "+------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = spark.read.option(\"header\", \"true\").csv(\"hdfs://rpi0:8020/data/ml-20m/ratings.csv\")\n",
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tuple of the `ratings` DataFrame represents one rating (`rating`) for one movie (`movieId`) by one user (`userId`). The ratings ranges from 0.5 stars (worst) up to 5.0 stars (best). \n",
    "\n",
    "We can also have look on the Schema of our dataset (column names and types) by using the `printSchema()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the `movies.csv` file. What kind of data is available and how does the schema look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Exercise 1: Which movies are the most popular ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the most popular movies, we are looking for the movies with the highest number of ratings. In this task, we assume the number of ratings as a representative for the most popular movies. To do this, we will perform the following *transformations* on the `ratings` DataFrame: \n",
    "- group by `movieId`\n",
    "- count the number of users (`userId`) associated with each movie \n",
    "- rename this column to `num_ratings`\n",
    "- sort by `num_ratings` in descending order \n",
    "\n",
    "We do these transformations in `PySpark` and store the DataFrame as `most_popular`. Have also a look on the [official pySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html).\n",
    "\n",
    "**HINT**:\n",
    "- Use `agg(count())` to perform an aggregate calculation on grouped data. \n",
    "- Don't forget that transformations are [lazy](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations) in spark. We need to call an action (e.g. `show()` for Dataframes, `take()` for RDD's) explicitly to see the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|movieId|num_ratings|\n",
      "+-------+-----------+\n",
      "|    296|      67310|\n",
      "|    356|      66172|\n",
      "|    318|      63366|\n",
      "|    593|      63299|\n",
      "|    480|      59715|\n",
      "+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "most_popular = ratings.groupBy(\"movieId\").agg(count(\"userId\")).withColumnRenamed(\"count(userId)\", \"num_ratings\").sort(desc(\"num_ratings\"))\n",
    "\n",
    "most_popular.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the resulting table only contains `movieId` and `num_ratings`. The title of the movie is stored in the `movies` DataFrame. So, we need an inner join of our `most_popular` DataFrame with the `movies` DataFrame on `movieId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m most_popular_movies \u001b[38;5;241m=\u001b[39m most_popular\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mmovies\u001b[49m, most_popular\u001b[38;5;241m.\u001b[39mmovieId \u001b[38;5;241m==\u001b[39m movies\u001b[38;5;241m.\u001b[39mmovieId)\n\u001b[1;32m      2\u001b[0m most_popular_movies\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_ratings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'movies' is not defined"
     ]
    }
   ],
   "source": [
    "most_popular_movies = most_popular.join(movies, most_popular.movieId == movies.movieId)\n",
    "most_popular_movies.select(\"title\", \"num_ratings\").show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of the most popular (or most rated) movies of our movielens dataset. Have you already watched all of them? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Exercise 2: Understanding RDD (Resilient Distributed Datasets) operations\n",
    "\n",
    "We will have a look on two core data abstractions of Spark, namely Dataframes and RDD's.\n",
    "RDDs and DataFrames can be created from external data sources (e.g. HDFS, SQL) or from internal process steps. Dataframes the are easiest abstraction. One can compare Dataframes with a traditional table with columns and rows, which is generally used for handling workflows with structured data. If the data is unstructured (has no schema) and the data needs to be manipulated in non-standard ways, one should use RDD's. Even though our data is structured, we will use some operations on RDD's to understand RDD transformations. \n",
    "\n",
    "Have a look on the Pyspark Documentation for RDD operations [PySpark Package](https://spark.apache.org/docs/1.5.1/api/python/pyspark.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cells and try to understand what map(), flatmap() and take() do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into RDD\n",
    "data = sc.textFile(\"hdfs://rpi0:8020/data/ml-20m/movies.csv\")\n",
    "\n",
    "# Split the RDD \n",
    "moviesRDD = data.map(lambda l: l.split(','))\n",
    "moviesRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "Reversed = moviesRDD.map(lambda m: m[::-1])\n",
    "Reversed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap\n",
    "words = moviesRDD.flatMap(lambda m: list(m))\n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Distinct Movie Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to combine these operations above by trying to extract all distinct movie genres in our Movielens data. You need to:\n",
    "- Read the csv file located in (\"ml-20m/movies.csv\")\n",
    "- Split the data and select the corresponding genre column via `map()`\n",
    "- `flatmap()` the data --> **Remark**: Be aware that a movie can contain several genres delimited by `('|')`\n",
    "- Print the results by`take()` the `distinct()` genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adventure', 'Children', 'Comedy', 'Romance', 'Drama', 'Action', ' The (1995)\"', 'Horror', 'Sci-Fi', 'IMAX']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/29 21:13:33 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "24/06/29 21:13:34 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "movies_rdd=sc.textFile(\"hdfs://rpi0:8020/data/ml-20m/movies.csv\")\n",
    "\n",
    "movies_split= movies_rdd.map(lambda lines:lines.split(','))\n",
    "genres=movies_split.map(lambda line:(line[2]))\n",
    "genre=genres.flatMap(lambda l:l.split('|'))\n",
    "genres_distinct=genre.distinct()\n",
    "\n",
    "print(genres_distinct.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Which movies have the highest ratings (in average)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. We want to see which movies are rated to be the best. You need to use the `ratings` DataFrame and: \n",
    "\n",
    "- Group by `movieId` \n",
    "- Calculate the average rating for each movie and rename this column to `avg_rating`\n",
    "- Sort by `avg_rating` in descending order \n",
    "- Join the resulting DataFrame with the `movies` DataFrame to get the movienames.\n",
    "\n",
    "\n",
    "**NOTE** Be sure that you read the movies file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rated = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will realize that all of the movies with `avg_rating` of exactly 5.0 have 2 or less `num_ratings` . We must investigate the distribution of `num_ratings` to only consider movies that have a minimum number of ratings. Calculate some summary statistics within Spark (consider `mean()`, `min()` and `max()`) and take a decent value to filter your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: What's the number of movies in each genre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we want to calculate the number of movies in each genre. This exercise is similar to the guided introduction exercise 2:\n",
    "\n",
    "- Read the csv file located in (\"ml-20m/movies.csv\")\n",
    "- Split the data and select the corresponding genre column via `map()`\n",
    "- `flatmap()` the data --> **Remark**: Be aware that a movie can contain several genres delimited by `('|')`\n",
    "- Have a look on the [official pySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html) and check what the `reduceByKey()` function do. This function is needed to sum up the number of movies in each genre. \n",
    "- Sort the results using the `sortBy()` function\n",
    "- Print the results by`take()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rdd ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Which movies are a matter of taste?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, movies are a matter of taste. There are for sure some movies, which you would rate with a 5 whereas your friend rates the same movie with a 2. These are the movies that divide your opinon. Try to find out, which movies belong to this category.\n",
    "\n",
    "**HINT**\n",
    "\n",
    "- We need to consider the standard deviation of the movie ratings\n",
    "- Also, try to consider only movies that have some minimum number of ratings (e.g. 700) \n",
    "- Join with the movies table to get the movie names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matterofTaste_movies = "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "238px",
    "width": "412px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
